---
title: "HW6"
author: "Erik Andersen"
date: "2023-12-01"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
here::i_am("HW6/HW6.Rmd")

# Load packages
pacman::p_load(tidyverse, magrittr, estimatr, broom, ri2)
```

```{r}
# Load data
vote_df = haven::read_dta(here::here("HW6", "data", "GriffithNoonen2022_Econ587.dta"))

# Clean names
vote_df = janitor::clean_names(vote_df)

# Add variables of interest for Did. 
# Post = after 2017 which is the year of interest
# Seattle = in seattle
# Treat = interaction of post and seattle
# city_cycle = interaction of city and cycle
vote_df = vote_df |> 
  mutate(post = if_else(election_year>= 2017, 1, 0),
                    seattle = if_else(city == 'Seattle', 1, 0),
                    treatment = post * seattle,
                    city = as.factor(city),
                    cycle = as.factor(cycle),
                    city_cycle = city:cycle) |> 
  select(post, seattle, treatment, everything())
```

### Question 1

#### a)

```{r}
# did with regular SE's 
did_reg_classical = lm_robust(candidates_ballot ~ post + seattle + treatment + at_large * special,
                              vote_df,
                              se_type = 'classical')
# Extract standard error of coefficient of interest
tidy(did_reg_classical) |> select(term, std.error, p.value) |> filter(term == 'treatment') # 0.958 

# Rerun with HC robust SEs
did_reg_hc = lm_robust(candidates_ballot ~ post + seattle + treatment + at_large * special,
                              vote_df,
                              se_type = 'stata')
# Extract standard error again
tidy(did_reg_hc) |> select(term, std.error, p.value) |> filter(term == 'treatment') # 0.987
rm(did_reg_hc)
```

#### b)

```{r}
# Construct residuals for both regressions. The lm_robust function doesn't calculat these automatically so we have to do it manually
vote_df = vote_df |> mutate(resids = candidates_ballot - did_reg_classical$fitted.values)

# Naive test for heteroskedasticity
# resids^2 ~ treatment
reg_hetero = lm_robust(I(resids^2) ~ treatment, 
                       vote_df,
                       se_type = 'classical')

tidy(reg_hetero)
rm(reg_hetero)
rm(did_reg_classical)
```

#### c)

```{r}
# Cluster by city_cycle
reg_city_cycle = lm_robust(candidates_ballot ~ post + seattle + treatment + at_large * special,
                    vote_df,
                    clusters = city_cycle,
                    se_type = 'CR2') # CR2 is stata standard errors

tidy(reg_city_cycle)
rm(reg_city_cycle)
```

#### d)

```{r}
# Cluster by only city
reg_city = lm_robust(candidates_ballot ~ post + seattle + treatment + at_large * special,
                    vote_df,
                    clusters = city, 
                    se_type = 'CR2') # CR2 is stata standard errors
tidy(reg_city)
rm(reg_city)

# Cluster by only cycle
reg_cycle = lm_robust(candidates_ballot ~ post + seattle + treatment + at_large * special,
                    vote_df,
                    clusters = cycle,
                    se_type = 'CR2') # CR2 is stata standard errors
tidy(reg_cycle)
rm(reg_cycle)
```

#### e)

```{r}
# Redo the earlier parts with the following new specifcation
# candidates_ballot = cycle_fixed_effct + city_fixed_effect + treatment 

# First redo part a 
# Classical errors
reg_two_way_classic = lm_robust(candidates_ballot ~ cycle + city + treatment + at_large * special, 
                                vote_df,
                                se_type = "classical")
tidy(reg_two_way_classic)
rm(reg_two_way_classic)
# Het robust errors
reg_two_way_hc = lm_robust(candidates_ballot ~ cycle + city + treatment + at_large * special, 
                                vote_df,
                                se_type = "stata")
tidy(reg_two_way_hc)
rm(reg_two_way_hc)

# Redo part c
# Clustered errors at city and cycle level
reg_two_way_city_cycle = lm_robust(candidates_ballot ~ cycle + city + treatment + at_large * special,
                    vote_df,
                    clusters = city_cycle,
                    se_type = 'CR2') # CR2 is stata standard errors

tidy(reg_two_way_city_cycle)

# Redo part d
# Cluster by city only 
reg_two_way_city = lm_robust(candidates_ballot ~ cycle + city + treatment + at_large * special,
                    vote_df,
                    clusters = city, 
                    se_type = 'CR2') # CR2 is stata standard errors
tidy(reg_two_way_city)

# Cluster by only cycle
reg_two_way_cycle = lm_robust(candidates_ballot ~ cycle + city + treatment + at_large * special,
                    vote_df,
                    clusters = cycle,
                    se_type = 'CR2') # CR2 is stata standard errors
tidy(reg_two_way_cycle)
```

#### f)

```{r}
# Cameron Miller two way clustering decomposition
# Variance for clustering only by city
var_city = reg_two_way_city$std.error["treatment"]^2

# Variance for clustering only by cycle
var_cycle = reg_two_way_cycle$std.error["treatment"]^2

# Variance for clustering at the intersection of city and cycle
var_intersection = reg_two_way_city_cycle$std.error["treatment"]^2

# Calculate two way clustering variance
se_two_way = sqrt(var_city + var_cycle - var_intersection)

rm(reg_two_way_city, reg_two_way_cycle, reg_two_way_city_cycle,
        var_city, var_cycle, var_intersection, se_two_way)
```

### Question 2

#### a)

```{r}
# Define randomization procedure
# This is all we need to do since we're doing a naive estimate
randomization_naive = declare_ra(N = nrow(vote_df), 
                                 m = 2)

# Do randomization inference
conduct_ri(candidates_ballot ~ cycle + city + treatment + at_large * special,
           declaration = randomization_naive,
           assignment = "treatment",
           data = vote_df,
           sharp_hypothesis = 0,
           progress_bar = TRUE)
```

#### b)

```{r}
# Now define randomization procedure based on clustering structure
randomization_intersection = declare_ra(N = nrow(vote_df), 
                                        m = 2,
                                        clusters = vote_df$city_cycle)

# Inference
conduct_ri(candidates_ballot ~ cycle + city + treatment + at_large * special,
           declaration = randomization_naive,
           assignment = "treatment",
           data = vote_df,
           sharp_hypothesis = 0,
           progress_bar = TRUE)
```

#### c)

```{r, cache=TRUE}
# Here we're manually doing the permutation. So first we're going to group gy city cycle and only return to variables we care about which are the variables in the twfe model
# We also drop seattle so we only  have untreated units
small_vote_df = vote_df |>
  group_by(city_cycle) |>
  summarise(treatment = mean(treatment),
            city = unique(city),
            cycle = unique(cycle),
            candidates_ballot = mean(candidates_ballot, na.rm = T),
            at_large = mean(at_large, na.rm = T),
            special = mean(special, na.rm = T)) |> 
  filter(treatment == 0)
  
# Create empty 148 by 148 matrix to put all permutations into
mean_effect = matrix(NA, nrow(small_vote_df), nrow(small_vote_df))

# Calculate mean effect for every permutation of two city-cycle pairs being treated
for (i in 1:(nrow(small_vote_df) - 1)) {
  # this only goes to 147 so we don't double count
  
  # Outer loop. Two loops just make all the permutations of treatments
  small_vote_df$treatment[i] = 1
  
  for (j in (i + 1):nrow(small_vote_df)) {
    # This starts at i+1 to avoid double counting
    
    # Inner loop
    small_vote_df$treatment[j] = 1
    
    # Calculate the treatment effect for treated and untreated groups
    effect = lm_robust( # I could use lm_robust_fit but I'm lazy so we can have a slow loop
      candidates_ballot ~ cycle + city + treatment + at_large * special,
      small_vote_df,
      clusters = cycle,
      se_type = 'CR2'
    )
    
    # Calculate the treatment effect for each permutation and store it in output matrix
    mean_effect[i, j] = coef(effect)["treatment"]
    
    # Reset treatment for inner loop. We need to do this so we only have two treated units at at time
    small_vote_df$treatment[j] = 0
  }
  
  # Reset outer loop so we only have two treated units at at time
  small_vote_df$treatment[i] = 0
}

# Now, to calculate the p value, we see what proportion of the treatment effects we generated above are greater than our observed treatment effect
greater = c(mean_effect) |> as_tibble() |> abs() |>  filter(value > 3.23227) 

(p_value = nrow(greater)/length(mean_effect))
rm(greater)
rm(p_value)
```

#### d)

```{r}
# Standardize generated treatment effects
mean_effect = mean_effect[!is.na(mean_effect)] |> c()
mean_effect_standardized = (mean_effect - mean(mean_effect)) / sd(mean_effect)

# plot the density
mean_effect_standardized |> 
  as_tibble() |> 
  ggplot(aes(value)) + 
    geom_density(color = 'red') + 
    stat_function(fun = dnorm, args = list(mean = mean(mean_effect), sd = sd(mean_effect)), color = 'blue') + 
    cowplot::theme_cowplot() + 
    xlab("Standardized Treatment Effect") + ylab("") + 
    labs(caption = "Red is data, blue is simulated distribution")
ggsave(here::here("HW6", "plots", "ks.test.jpg"))

# Test formally if densities are the same with ks density
ks.test(mean_effect_standardized, "pnorm", mean = 0, sd = 1)
```

